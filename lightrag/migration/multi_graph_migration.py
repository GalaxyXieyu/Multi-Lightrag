"""
å¤šå›¾è°±æ¶æ„æ•°æ®è¿ç§»è„šæœ¬
"""

import os
import json
import shutil
import asyncio
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List
from dataclasses import asdict

from ..models.multi_graph import GraphMetadata, GraphStatus
from ..base import DocProcessingStatus, DocStatus

logger = logging.getLogger(__name__)


class MultiGraphMigrator:
    """å¤šå›¾è°±æ¶æ„è¿ç§»å™¨"""
    
    def __init__(self, working_dir: str = "./dickens", graphs_dir: str = "./graphs"):
        self.working_dir = Path(working_dir)
        self.graphs_dir = Path(graphs_dir)
        self.backup_dir = None
        
    async def migrate_to_multi_graph(self, default_graph_name: str = "Default Graph") -> bool:
        """
        è¿ç§»ç°æœ‰æ•°æ®åˆ°å¤šå›¾è°±æ¶æ„
        
        Args:
            default_graph_name: é»˜è®¤å›¾è°±åç§°
            
        Returns:
            bool: è¿ç§»æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹å¤šå›¾è°±æ¶æ„è¿ç§»...")
            
            # 1. æ£€æŸ¥ç°æœ‰æ•°æ®
            if not await self._check_existing_data():
                logger.info("æ²¡æœ‰å‘ç°ç°æœ‰æ•°æ®ï¼Œè·³è¿‡è¿ç§»")
                return True
            
            # 2. å¤‡ä»½ç°æœ‰æ•°æ®
            await self._backup_existing_data()
            
            # 3. åˆ›å»ºå›¾è°±ç›®å½•ç»“æ„
            await self._create_graphs_directory()
            
            # 4. åˆ›å»ºé»˜è®¤å›¾è°±
            default_graph = await self._create_default_graph(default_graph_name)
            
            # 5. è¿ç§»æ•°æ®æ–‡ä»¶
            await self._migrate_data_files(default_graph)
            
            # 6. æ›´æ–°æ•°æ®æ ¼å¼
            await self._update_data_format(default_graph)
            
            # 7. éªŒè¯è¿ç§»ç»“æœ
            await self._verify_migration(default_graph)
            
            logger.info("âœ… å¤šå›¾è°±æ¶æ„è¿ç§»å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»å¤±è´¥: {e}")
            await self._rollback_migration()
            return False
    
    async def _check_existing_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰æ•°æ®"""
        if not self.working_dir.exists():
            return False
        
        # æ£€æŸ¥å…³é”®æ–‡ä»¶
        key_files = [
            "kv_store.json",
            "doc_status.json",
            "entities_vdb.pkl",
            "relationships_vdb.pkl"
        ]
        
        existing_files = []
        for file_name in key_files:
            file_path = self.working_dir / file_name
            if file_path.exists():
                existing_files.append(file_name)
        
        if existing_files:
            logger.info(f"å‘ç°ç°æœ‰æ•°æ®æ–‡ä»¶: {existing_files}")
            return True
        
        return False
    
    async def _backup_existing_data(self):
        """å¤‡ä»½ç°æœ‰æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.backup_dir = self.working_dir.parent / f"backup_{timestamp}"
        
        logger.info(f"å¤‡ä»½ç°æœ‰æ•°æ®åˆ°: {self.backup_dir}")
        shutil.copytree(self.working_dir, self.backup_dir)
        
    async def _create_graphs_directory(self):
        """åˆ›å»ºå›¾è°±ç›®å½•ç»“æ„"""
        self.graphs_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"åˆ›å»ºå›¾è°±ç›®å½•: {self.graphs_dir}")
        
    async def _create_default_graph(self, name: str) -> GraphMetadata:
        """åˆ›å»ºé»˜è®¤å›¾è°±"""
        graph_id = "default"
        graph_dir = self.graphs_dir / graph_id
        graph_dir.mkdir(parents=True, exist_ok=True)
        
        default_graph = GraphMetadata(
            graph_id=graph_id,
            name=name,
            description="Migrated from existing LightRAG data",
            working_dir=str(graph_dir),
            status=GraphStatus.ACTIVE,
            is_active=True
        )
        
        # ä¿å­˜å›¾è°±é…ç½®
        await self._save_graph_config(default_graph)
        
        logger.info(f"åˆ›å»ºé»˜è®¤å›¾è°±: {name} (ID: {graph_id})")
        return default_graph
        
    async def _save_graph_config(self, graph: GraphMetadata):
        """ä¿å­˜å›¾è°±é…ç½®"""
        config_file = self.graphs_dir / "graphs_config.json"
        
        # åŠ è½½ç°æœ‰é…ç½®
        config = {}
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
        
        # æ›´æ–°é…ç½®
        config[graph.graph_id] = graph.to_dict()
        
        # ä¿å­˜é…ç½®
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    
    async def _migrate_data_files(self, graph: GraphMetadata):
        """è¿ç§»æ•°æ®æ–‡ä»¶"""
        graph_dir = Path(graph.working_dir)
        
        # éœ€è¦è¿ç§»çš„æ–‡ä»¶åˆ—è¡¨
        files_to_migrate = [
            "kv_store.json",
            "text_chunks.json", 
            "entities_vdb.pkl",
            "relationships_vdb.pkl",
            "graph.graphml",
            "doc_status.json",
            "kv_store_llm_response_cache.json"
        ]
        
        migrated_files = []
        for file_name in files_to_migrate:
            source_file = self.working_dir / file_name
            target_file = graph_dir / file_name
            
            if source_file.exists():
                shutil.copy2(source_file, target_file)
                migrated_files.append(file_name)
                logger.info(f"è¿ç§»æ–‡ä»¶: {file_name}")
        
        logger.info(f"å…±è¿ç§» {len(migrated_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    
    async def _update_data_format(self, graph: GraphMetadata):
        """æ›´æ–°æ•°æ®æ ¼å¼ï¼Œæ·»åŠ graph_idå­—æ®µ"""
        graph_dir = Path(graph.working_dir)
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€æ•°æ®
        await self._update_doc_status_format(graph_dir, graph.graph_id)
        
        # æ›´æ–°å®ä½“æ•°æ®æ ¼å¼
        await self._update_entities_format(graph_dir, graph.graph_id)
        
        # æ›´æ–°å…³ç³»æ•°æ®æ ¼å¼
        await self._update_relationships_format(graph_dir, graph.graph_id)
        
        logger.info("æ•°æ®æ ¼å¼æ›´æ–°å®Œæˆ")
    
    async def _update_doc_status_format(self, graph_dir: Path, graph_id: str):
        """æ›´æ–°æ–‡æ¡£çŠ¶æ€æ•°æ®æ ¼å¼"""
        doc_status_file = graph_dir / "doc_status.json"
        if not doc_status_file.exists():
            return
        
        try:
            with open(doc_status_file, 'r', encoding='utf-8') as f:
                doc_data = json.load(f)
            
            # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ graph_id
            updated_data = {}
            for doc_id, doc_info in doc_data.items():
                if isinstance(doc_info, dict):
                    doc_info['graph_id'] = graph_id
                    doc_info['graph_name'] = None  # å°†åœ¨åç»­å¡«å……
                updated_data[doc_id] = doc_info
            
            # ä¿å­˜æ›´æ–°åçš„æ•°æ®
            with open(doc_status_file, 'w', encoding='utf-8') as f:
                json.dump(updated_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"æ›´æ–°äº† {len(updated_data)} ä¸ªæ–‡æ¡£çš„çŠ¶æ€æ ¼å¼")
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡æ¡£çŠ¶æ€æ ¼å¼å¤±è´¥: {e}")
    
    async def _update_entities_format(self, graph_dir: Path, graph_id: str):
        """æ›´æ–°å®ä½“æ•°æ®æ ¼å¼"""
        # æ³¨æ„ï¼šå®ä½“æ•°æ®é€šå¸¸å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œæ ¼å¼æ›´æ–°å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
        # è¿™é‡Œå…ˆè®°å½•æ—¥å¿—ï¼Œå…·ä½“å®ç°å–å†³äºå­˜å‚¨æ ¼å¼
        logger.info(f"å®ä½“æ•°æ®æ ¼å¼æ›´æ–° (graph_id: {graph_id})")
    
    async def _update_relationships_format(self, graph_dir: Path, graph_id: str):
        """æ›´æ–°å…³ç³»æ•°æ®æ ¼å¼"""
        # æ³¨æ„ï¼šå…³ç³»æ•°æ®é€šå¸¸å­˜å‚¨åœ¨å›¾æ•°æ®åº“ä¸­ï¼Œæ ¼å¼æ›´æ–°å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
        # è¿™é‡Œå…ˆè®°å½•æ—¥å¿—ï¼Œå…·ä½“å®ç°å–å†³äºå­˜å‚¨æ ¼å¼
        logger.info(f"å…³ç³»æ•°æ®æ ¼å¼æ›´æ–° (graph_id: {graph_id})")
    
    async def _verify_migration(self, graph: GraphMetadata):
        """éªŒè¯è¿ç§»ç»“æœ"""
        graph_dir = Path(graph.working_dir)
        
        # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = ["doc_status.json"]
        missing_files = []
        
        for file_name in required_files:
            if not (graph_dir / file_name).exists():
                missing_files.append(file_name)
        
        if missing_files:
            raise Exception(f"è¿ç§»éªŒè¯å¤±è´¥ï¼Œç¼ºå°‘æ–‡ä»¶: {missing_files}")
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_file = self.graphs_dir / "graphs_config.json"
        if not config_file.exists():
            raise Exception("å›¾è°±é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
        
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        if graph.graph_id not in config:
            raise Exception(f"å›¾è°±é…ç½®ä¸­ç¼ºå°‘é»˜è®¤å›¾è°±: {graph.graph_id}")
        
        logger.info("âœ… è¿ç§»éªŒè¯é€šè¿‡")
    
    async def _rollback_migration(self):
        """å›æ»šè¿ç§»"""
        if self.backup_dir and self.backup_dir.exists():
            logger.info(f"å›æ»šè¿ç§»ï¼Œä»å¤‡ä»½æ¢å¤: {self.backup_dir}")
            
            # åˆ é™¤å¯èƒ½åˆ›å»ºçš„å›¾è°±ç›®å½•
            if self.graphs_dir.exists():
                shutil.rmtree(self.graphs_dir)
            
            # æ¢å¤åŸå§‹æ•°æ®
            if self.working_dir.exists():
                shutil.rmtree(self.working_dir)
            shutil.copytree(self.backup_dir, self.working_dir)
            
            logger.info("å›æ»šå®Œæˆ")
        else:
            logger.warning("æ²¡æœ‰å¤‡ä»½æ•°æ®ï¼Œæ— æ³•å›æ»š")


async def run_migration(working_dir: str = "./dickens", graphs_dir: str = "./graphs"):
    """è¿è¡Œè¿ç§»è„šæœ¬"""
    migrator = MultiGraphMigrator(working_dir, graphs_dir)
    success = await migrator.migrate_to_multi_graph()
    
    if success:
        print("âœ… å¤šå›¾è°±æ¶æ„è¿ç§»æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ å›¾è°±æ•°æ®ç›®å½•: {graphs_dir}")
        print(f"ğŸ“¦ åŸå§‹æ•°æ®å¤‡ä»½: {migrator.backup_dir}")
    else:
        print("âŒ è¿ç§»å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    
    return success


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œè¿ç§»
    asyncio.run(run_migration())
